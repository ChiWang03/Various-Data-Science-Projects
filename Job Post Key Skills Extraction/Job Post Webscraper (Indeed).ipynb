{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for mac64 chromedriver:76.0.3809.126 in cache\n",
      "Driver found in /Users/chi/.wdm/chromedriver/76.0.3809.126/mac64/chromedriver\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#!pip install selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install webdriver-manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "\n",
    "# driver using chrome \n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data off of Indeed for Data Scientist and Machine Learning Engineer job posts \n",
    "\n",
    "* I decided not only to include Data Scientist job posts but job titles such as machine learning engineer and machine learning developer. There are many jobs out there that are similar to Data Scientist and will definitely help in  in collecting more data for a better performance when modelling. The quality of data that goes in the model will reflect the quality of the modelling results vastly. \n",
    "\n",
    "* I will be only scraping from Indeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets define a use_soup function so we can use it everytime it needs to ead a url\n",
    "def use_soup(url):\n",
    "    driver.get(url)\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return soup \n",
    "    \n",
    "# Note: the url scraped in this case is a search on the indeed website for data scientist jobs\n",
    "# The location of the search can be altered easily if needed\n",
    "\n",
    "vancouver = 'Vancouver%2C+BC'\n",
    "toronto = 'Toronto%2C+ON'\n",
    "montreal = 'Montreal%2C+QC'\n",
    "ottawa = 'Ottawa%2C+ON'\n",
    "calgary = 'Calgary%2C+AB'\n",
    "\n",
    "# This is the basic url that lets us input a query and a location\n",
    "basic_url = 'https://www.indeed.ca/jobs?q={}&l={}'\n",
    "\n",
    "data_scientist = 'data+scientist'\n",
    "machine_learning_engineer = 'machine+learning+engineer'\n",
    "\n",
    "\n",
    "\n",
    "# extract_post_links takes two inputs location such as vancouver, toronto, etc\n",
    "# and it also takes the number of pages num_pages of job posts after searching for data scientist\n",
    "\n",
    "def extract_post_links(position,location, num_pages):\n",
    "    url_posts = []\n",
    "    titles = []\n",
    "    # pages*20 since each page has roughly 20 job posts \n",
    "    total_posts = num_pages*20\n",
    "    \n",
    "    # we are working with an increment of 20 since there are 20 items per page \n",
    "    # suppose there are 12 pages => 220 jobs , which makes total post = 240 since python's range is not inclusive\n",
    "    for item_num in range(0,total_posts,20):\n",
    "        if item_num ==0:\n",
    "            # the first page is slightly different where there is no '&start={}', an error will rise when included\n",
    "            url = basic_url.format(position,location)\n",
    "\n",
    "        else:\n",
    "            url = basic_url.format(position,location)+'&start='+str(item_num)\n",
    "\n",
    "        soup = use_soup(url)\n",
    "        # the links are all stored under the tag div class = 'title' when inspected\n",
    "        job_post_urls = soup.find_all('div', {'class': 'title'})\n",
    "    \n",
    "    \n",
    "        # save the url for each post in url_posts (the list defined above)\n",
    "        for link in job_post_urls:\n",
    "            partial = link.a.get('href')\n",
    "            job_title = link.a.get('title')\n",
    "            # we need to add https://ca.indeed.com before the url obtained from the href \n",
    "            # this gives us a complete url\n",
    "            post_url = 'https://ca.indeed.com' + partial\n",
    "            titles.append(job_title)\n",
    "            url_posts.append(post_url)\n",
    "            \n",
    "        \n",
    "    return (url_posts,titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the extract_text function is used to extract the \n",
    "# job description from clicking on the url we obtained from the extract_post_links function\n",
    "\n",
    "def extract_text(url_posts):\n",
    "    extracted = []\n",
    "    for link in url_posts:\n",
    "        soup = use_soup(link)\n",
    "        extracted_information = soup.find('div', {'id': 'jobDescriptionText'})\n",
    "        extracted_text = extracted_information.get_text()\n",
    "        extracted.append(extracted_text)\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have already manually checked all the urls in the different locations \n",
    "# since they all have different number of pages \n",
    "# this is subject to change since employers can take down job posts\n",
    "\n",
    "# ds for data scientist, and ml for machine learning engineer\n",
    "\n",
    "ds_van, ds_van_titles = extract_post_links(data_scientist,vancouver, 12)\n",
    "ml_van, ml_van_titles  = extract_post_links(machine_learning_engineer,vancouver, 8)\n",
    "\n",
    "# Collect Toronto Links\n",
    "ds_toronto, ds_toronto_titles = extract_post_links(data_scientist,vancouver,12)\n",
    "ml_toronto, ml_toronto_titles =extract_post_links(machine_learning_engineer,toronto, 10)\n",
    "\n",
    "# Collect Ottawa Links\n",
    "ds_ottawa, ds_ottawa_titles = extract_post_links(data_scientist,ottawa,3)\n",
    "\n",
    "# Collect Montreal Links\n",
    "ds_montreal, ds_montreal_titles = extract_post_links(data_scientist,montreal,9)\n",
    "ml_montreal, ml_montreal_titles =extract_post_links(machine_learning_engineer,montreal, 8)\n",
    "\n",
    "# Collect Calgary Links \n",
    "ds_Calgary, ds_Calgary_titles = extract_post_links(data_scientist,calgary,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_van_text = extract_text(ds_van)\n",
    "ml_van_text = extract_text(ml_van)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_toronto_text = extract_text(ds_toronto)\n",
    "ml_toronto_text = extract_text(ml_toronto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ottawa_text = extract_text(ds_ottawa)\n",
    "ds_calgary_text = extract_text(ds_Calgary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I spelled montreal wrong, but its just defining a variable \n",
    "# It takes some time to extract data so we won't re run it again \n",
    "ds_montral_text = extract_text(ds_montreal)\n",
    "ml_montral_text = extract_text(ml_montreal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets quickly check that the number of rows are the same \n",
    "* It may be that while I was doing this the job post was taken down and url may not work\n",
    "* There is usually no need to check, if a more complicated try and except is implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 302 302\n",
      "194 194 194\n",
      "305 305 305\n",
      "252 252 252\n",
      "53 53 53\n",
      "221 221 221\n",
      "202 202 202\n",
      "86 86 86\n"
     ]
    }
   ],
   "source": [
    "print(len(ds_van), len(ds_van_titles), len(ds_van_text))\n",
    "print(len(ml_van), len(ml_van_titles), len(ml_van_text))\n",
    "\n",
    "print(len(ds_toronto), len(ds_toronto_titles), len(ds_toronto_text))\n",
    "print(len(ml_toronto), len(ml_toronto_titles), len(ml_toronto_text))\n",
    "\n",
    "print(len(ds_ottawa), len(ds_ottawa_titles), len(ds_ottawa_text))\n",
    "\n",
    "print(len(ds_montreal), len(ds_montreal_titles), len(ds_montral_text))\n",
    "print(len(ml_montreal), len(ml_montreal_titles), len(ml_montral_text))\n",
    "\n",
    "print(len(ds_Calgary),len(ds_Calgary_titles), len(ds_calgary_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataframe and save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "vancouver_list1 = pd.DataFrame(\n",
    "    {'Job Title': ds_van_titles,\n",
    "     'Description': ds_van_text,\n",
    "     'Location': ['Vancouver']*302,\n",
    "     'url': ds_van\n",
    "    })\n",
    "\n",
    "vancouver_list2 = pd.DataFrame(\n",
    "    {'Job Title': ml_van_titles,\n",
    "     'Description': ml_van_text,\n",
    "     'Location': ['Vancouver']*194,\n",
    "     'url': ml_van\n",
    "    })\n",
    "\n",
    "toronto_list1 = pd.DataFrame(\n",
    "    {'Job Title': ds_toronto_titles,\n",
    "     'Description': ds_toronto_text,\n",
    "     'Location': ['Toronto']*305,\n",
    "     'url':ds_toronto\n",
    "    })\n",
    "\n",
    "toronto_list2 = pd.DataFrame(\n",
    "    {'Job Title': ml_toronto_titles,\n",
    "     'Description': ml_toronto_text,\n",
    "     'Location': ['Toronto']*252,\n",
    "     'url':ml_toronto\n",
    "    })\n",
    "\n",
    "ottawa_list = pd.DataFrame(\n",
    "    {'Job Title': ds_ottawa_titles,\n",
    "     'Description': ds_ottawa_text,\n",
    "     'Location': ['Ottawa']*53,\n",
    "     'url':ds_ottawa\n",
    "    })\n",
    "\n",
    "montreal_list1 = pd.DataFrame(\n",
    "    {'Job Title': ds_montreal_titles,\n",
    "     'Description': ds_montral_text,\n",
    "     'Location': ['Montreal']*221,\n",
    "     'url':ds_montreal\n",
    "    })\n",
    "\n",
    "montreal_list2 = pd.DataFrame(\n",
    "    {'Job Title': ml_montreal_titles,\n",
    "     'Description': ml_montral_text,\n",
    "     'Location': ['Montreal']*202,\n",
    "     'url':ml_montreal\n",
    "    })\n",
    "\n",
    "calgary_list = pd.DataFrame(\n",
    "    {'Job Title': ds_Calgary_titles,\n",
    "     'Description': ds_calgary_text,\n",
    "     'Location': ['Calgary']*86,\n",
    "     'url':ds_Calgary\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [vancouver_list1, vancouver_list2, \n",
    "                      toronto_list1,toronto_list2,\n",
    "                      ottawa_list,montreal_list1,\n",
    "                      montreal_list2, calgary_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(frames)\n",
    "df = df.reset_index(inplace = False, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('job_postings.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
